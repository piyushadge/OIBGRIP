{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a05161ba",
   "metadata": {},
   "source": [
    "## DL Assignment No. 05\n",
    "5. Implement the Continuous Bag of Words (CBOW) Model. Stages can be:\n",
    "\n",
    "a. Data preparation\n",
    "\n",
    "b. Generate training data\n",
    "\n",
    "c. Train model\n",
    "\n",
    "d. Output\n",
    "\n",
    "https://www.kaggle.com/code/aggarwalrahul/nlp-word-embedding-continuous-bag-of-words-cbow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb486196",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c65407f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = \"\"\" The speed of transmission is an important point of difference between the two viruses. Influenza has a shorter median incubation period (the time from infection to appearance of symptoms) and a shorter serial interval (the time between successive cases) than COVID-19 virus. The serial interval for COVID-19 virus is estimated to be 5-6 days, while for influenza virus, the serial interval is 3 days. This means that influenza can spread faster than COVID-19. \n",
    "Further, transmission in the first 3-5 days of illness, or potentially pre-symptomatic transmission –transmission of the virus before the appearance of symptoms – is a major driver of transmission for influenza. In contrast, while we are learning that there are people who can shed COVID-19 virus 24-48 hours prior to symptom onset, at present, this does not appear to be a major driver of transmission. \n",
    "The reproductive number – the number of secondary infections generated from one infected individual – is understood to be between 2 and 2.5 for COVID-19 virus, higher than for influenza. However, estimates for both COVID-19 and influenza viruses are very context and time-specific, making direct comparisons more difficult.  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82c66408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a. Data preparation\n",
    "# remove special characters from text\n",
    "sentences = re.sub('[^A-Za-z0-9]+', ' ', sentences)\n",
    "#sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "855ee2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make all words lower case\n",
    "sentences = sentences.lower()\n",
    "#sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97347833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the vocabulary\n",
    "words = sentences.split()\n",
    "vocab = set(words)\n",
    "vocab_size = len(vocab)\n",
    "#vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a04218a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexing the Words\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "ix_to_word = {i: word for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e553a108",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ix_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71e9efb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['the', 'speed', 'transmission', 'is'], 'of'), (['speed', 'of', 'is', 'an'], 'transmission'), (['of', 'transmission', 'an', 'important'], 'is'), (['transmission', 'is', 'important', 'point'], 'an'), (['is', 'an', 'point', 'of'], 'important')]\n"
     ]
    }
   ],
   "source": [
    "# b. Generate training data\n",
    "data = []\n",
    "for i in range(2, len(words) - 2):\n",
    "    context = [words[i - 2], words[i - 1], words[i + 1], words[i + 2]]\n",
    "    target = words[i]\n",
    "    data.append((context, target))\n",
    "print(data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b732b548",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.random.random_sample((vocab_size, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "279999e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(context_idxs, theta):\n",
    "    m = embeddings[context_idxs].reshape(1, -1)\n",
    "    n = m.dot(theta)\n",
    "    o = np.log(np.exp(n-np.max(n)) / np.exp(n-np.max(n)).sum())\n",
    "    return m, n, o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "974cabeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(preds, theta, target_idxs):\n",
    "    m, n, o = preds\n",
    "    out = np.zeros_like(n)\n",
    "    out[np.arange(len(n)),target_idxs] = 1\n",
    "    softmax = np.exp(n) / np.exp(n).sum(axis=-1,keepdims=True)\n",
    "    dlog = (- out + softmax) / n.shape[0]\n",
    "    dw = m.T.dot(dlog)\n",
    "    return dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38ee52a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch - 0 \tloss - 5.336473570773312\n",
      "epoch - 1 \tloss - 3.721455354345014\n",
      "epoch - 2 \tloss - 4.185769530924911\n",
      "epoch - 3 \tloss - 5.267146301078617\n",
      "epoch - 4 \tloss - 4.096753216140029\n",
      "epoch - 5 \tloss - 5.4119348438620625\n",
      "epoch - 6 \tloss - 3.634939371180429\n",
      "epoch - 7 \tloss - 4.762962100214823\n",
      "epoch - 8 \tloss - 4.793614605089183\n",
      "epoch - 9 \tloss - 3.6810390530527903\n",
      "epoch - 10 \tloss - 5.28522726450934\n",
      "epoch - 11 \tloss - 5.588577406138822\n",
      "epoch - 12 \tloss - 3.3947510593754284\n",
      "epoch - 13 \tloss - 8.061769351899502\n",
      "epoch - 14 \tloss - 3.1395575890835072\n",
      "epoch - 15 \tloss - 3.1601178332129822\n",
      "epoch - 16 \tloss - 4.497819447270813\n",
      "epoch - 17 \tloss - 4.288797635122561\n",
      "epoch - 18 \tloss - 4.740924554864281\n",
      "epoch - 19 \tloss - 2.1283893015940447\n",
      "epoch - 20 \tloss - 3.6754438252418415\n",
      "epoch - 21 \tloss - 3.6813955124767284\n",
      "epoch - 22 \tloss - 3.6052383071790195\n",
      "epoch - 23 \tloss - 2.4416638396943617\n",
      "epoch - 24 \tloss - 1.8851089402375263\n",
      "epoch - 25 \tloss - 2.3087246871909843\n",
      "epoch - 26 \tloss - 0.9242279134497954\n",
      "epoch - 27 \tloss - 1.7780759574369915\n",
      "epoch - 28 \tloss - 1.933987950109674\n",
      "epoch - 29 \tloss - 2.3848741890495875\n",
      "epoch - 30 \tloss - 1.9198816136716146\n",
      "epoch - 31 \tloss - 1.943162449160059\n",
      "epoch - 32 \tloss - 2.5415671262348107\n",
      "epoch - 33 \tloss - 2.213466115572346\n",
      "epoch - 34 \tloss - 2.7498308041816224\n",
      "epoch - 35 \tloss - 1.7038901769811177\n",
      "epoch - 36 \tloss - 2.7840315961347253\n",
      "epoch - 37 \tloss - 1.7613919856517515\n",
      "epoch - 38 \tloss - 0.31560002294402145\n",
      "epoch - 39 \tloss - 0.48057996031751055\n",
      "epoch - 40 \tloss - 0.3455442930232451\n",
      "epoch - 41 \tloss - 0.8363519274315828\n",
      "epoch - 42 \tloss - 0.5211864146799188\n",
      "epoch - 43 \tloss - 1.130144496674439\n",
      "epoch - 44 \tloss - 1.245600322673399\n",
      "epoch - 45 \tloss - 0.08519502692186416\n",
      "epoch - 46 \tloss - 0.2201015712850742\n",
      "epoch - 47 \tloss - 0.35911555315807936\n",
      "epoch - 48 \tloss - 2.784849295767658\n",
      "epoch - 49 \tloss - 1.7988855404360233\n",
      "epoch - 50 \tloss - 0.3858985345791377\n",
      "epoch - 51 \tloss - 0.6092895960884775\n",
      "epoch - 52 \tloss - 2.1433735965971046\n",
      "epoch - 53 \tloss - 1.4813260961298418\n",
      "epoch - 54 \tloss - 1.1963211189501457\n",
      "epoch - 55 \tloss - 1.959628504035179\n",
      "epoch - 56 \tloss - 0.30855541062759867\n",
      "epoch - 57 \tloss - 0.8616227487596279\n",
      "epoch - 58 \tloss - 0.7238797471487306\n",
      "epoch - 59 \tloss - 0.7674208995863832\n",
      "epoch - 60 \tloss - 0.4793374818894156\n",
      "epoch - 61 \tloss - 0.2877753571543489\n",
      "epoch - 62 \tloss - 2.083864138734878\n",
      "epoch - 63 \tloss - 0.8585667634572194\n",
      "epoch - 64 \tloss - 2.1081800627851868\n",
      "epoch - 65 \tloss - 1.5100953582424306\n",
      "epoch - 66 \tloss - 1.4061505957028901\n",
      "epoch - 67 \tloss - 0.9658778748275262\n",
      "epoch - 68 \tloss - 2.042612178617637\n",
      "epoch - 69 \tloss - 0.9211511792440622\n",
      "epoch - 70 \tloss - 1.2372049382890835\n",
      "epoch - 71 \tloss - 1.2060480595718586\n",
      "epoch - 72 \tloss - 0.969853643300997\n",
      "epoch - 73 \tloss - 0.25540863457506136\n",
      "epoch - 74 \tloss - 0.41821370640237054\n",
      "epoch - 75 \tloss - 1.0175122589523937\n",
      "epoch - 76 \tloss - 1.2245330424493157\n",
      "epoch - 77 \tloss - 1.4683931938643677\n",
      "epoch - 78 \tloss - 0.7209151363858955\n",
      "epoch - 79 \tloss - 0.5304505319726508\n"
     ]
    }
   ],
   "source": [
    "# c. Train model\n",
    "theta = np.random.uniform(-1, 1, (40, vocab_size))\n",
    "epoch_losses = {}\n",
    "for epoch in range(80):\n",
    "    losses = []\n",
    "    for context, target in data:\n",
    "        context_idxs = np.array([word_to_ix[w] for w in context])\n",
    "        preds = forward(context_idxs, theta)\n",
    "        target_idxs = np.array([word_to_ix[target]])\n",
    "        loss = -preds[-1][range(len(target_idxs)), target_idxs].sum() / len(preds[-1][range(len(target_idxs)), target_idxs])\n",
    "        losses.append(loss)\n",
    "        grad = backward(preds, theta, target_idxs)\n",
    "        theta = theta - (grad*0.03)\n",
    "    epoch_losses[epoch] = losses\n",
    "    print('epoch -', epoch, '\\tloss -', losses[epoch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32df9272",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(words):\n",
    "    context_idxs = np.array([word_to_ix[w] for w in words])\n",
    "    preds = forward(context_idxs, theta)\n",
    "    word = ix_to_word[np.argmax(preds[-1])]\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfebfcb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9685863874345549"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def accuracy():\n",
    "    wrong = 0\n",
    "    for context, target in data:\n",
    "        if(predict(context) != target):\n",
    "            wrong += 1\n",
    "    return (1 - (wrong / len(data)))\n",
    "accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38bc6567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'an'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# d. Output\n",
    "predict(['transmission', 'is', 'important', 'point'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
